{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_board(fen):\n",
    "    board = []\n",
    "    for row in fen.split('/'):\n",
    "        brow = []\n",
    "        for c in row:\n",
    "            if c == ' ':\n",
    "                break\n",
    "            elif c in '12345678':\n",
    "                brow.extend( ['--'] * int(c) )\n",
    "            elif c == 'p':\n",
    "                brow.append( 'bp' )\n",
    "            elif c == 'P':\n",
    "                brow.append( 'wp' )\n",
    "            elif c > 'Z':\n",
    "                brow.append( 'b'+c.upper() )\n",
    "            else:\n",
    "                brow.append( 'w'+c )\n",
    "\n",
    "        board.append( brow )\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envi():\n",
    "    def __init__(self,intial_fen):\n",
    "        if intial_fen == \"\":\n",
    "            self.board = chess.Board()\n",
    "        else:\n",
    "            self.board = chess.Board(intial_fen)\n",
    "        fen=self.board.fen()\n",
    "        text=fen_to_board(fen)\n",
    "        self.tokenizer=Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(text)\n",
    "\n",
    "    def step(self,action):\n",
    "        self.board.push(action)\n",
    "        self.brd=self.state()\n",
    "        if self.board.is_checkmate():\n",
    "            return 10000,self.brd,1\n",
    "        elif self.board.is_check():\n",
    "            return 100,self.brd,0\n",
    "        else:\n",
    "            return 0,self.brd,0\n",
    "        \n",
    "    def state(self):\n",
    "        fen=self.board.fen()\n",
    "        text=fen_to_board(fen)\n",
    "        seq=self.tokenizer.texts_to_sequences(text)\n",
    "        self.brd=np.array(seq).reshape(64)\n",
    "        return self.brd\n",
    "        \n",
    "    def legal(self):\n",
    "        self.pmoves=[x for x in self.board.legal_moves]\n",
    "        return self.pmoves\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board.reset()\n",
    "        return self.board.fen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "import time\n",
    "import random\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    Input(shape=(64,)),                      \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=1, activation='linear'),\n",
    "    ])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=(64,)),                      \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=1, activation='linear'),\n",
    "    ])\n",
    "\n",
    "optimizer =Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,385\n",
      "Trainable params: 8,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    y_targets =rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),tf.cast(actions, tf.int32)], axis=1))    \n",
    "    loss = MSE(y_targets, q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    utils.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games=1\n",
    "num_moves=1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av=100\n",
    "epsilon=1.0\n",
    "\n",
    "target_update_interval=100\n",
    "\n",
    "memory_buffer=deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "target_q_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getaction(env,epsilon):\n",
    "    q = []\n",
    "    for i in env.legal():\n",
    "        new_state=env.step(i)[1]\n",
    "        state_qn = np.expand_dims(new_state, axis=0)\n",
    "        env.board.pop()\n",
    "        q.append(q_network(state_qn))\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.legal()[np.random.randint(len(env.legal()))]\n",
    "    else:\n",
    "        return env.legal()[np.argmax(q)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qupdate(t):\n",
    "    if t % target_update_interval == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(memory_buffer):\n",
    "    batch=random.sample(memory_buffer,10)\n",
    "    batch=tf.convert_to_tensor(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_eps(eps):\n",
    "    return eps * 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert Python sequence with mixed types to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m update\u001b[39m=\u001b[39mqupdate(t)\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m update:\n\u001b[1;32m---> 14\u001b[0m     experiences\u001b[39m=\u001b[39mget_experiences(memory_buffer)\n\u001b[0;32m     16\u001b[0m     agent_learn(experiences,GAMMA)\n\u001b[0;32m     18\u001b[0m     env\u001b[39m.\u001b[39mboard\n",
      "Cell \u001b[1;32mIn[220], line 3\u001b[0m, in \u001b[0;36mget_experiences\u001b[1;34m(memory_buffer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_experiences\u001b[39m(memory_buffer):\n\u001b[0;32m      2\u001b[0m     batch\u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39msample(memory_buffer,\u001b[39m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     batch\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mconvert_to_tensor(batch)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor."
     ]
    }
   ],
   "source": [
    "for i in range(num_games):\n",
    "    env=Envi(\"\")\n",
    "    state=env.state()\n",
    "    total_points=0\n",
    "\n",
    "    for t in range(1,num_moves):\n",
    "        action=getaction(env,epsilon)\n",
    "        next_state,reward,done=env.step(action)\n",
    "        memory_buffer.append((state,action,reward,next_state,done))\n",
    "\n",
    "        update=qupdate(t)\n",
    "\n",
    "        if update:\n",
    "            experiences=get_experiences(memory_buffer)\n",
    "\n",
    "            agent_learn(experiences,GAMMA)\n",
    "\n",
    "            env.board\n",
    "\n",
    "        state=next_state\n",
    "        total_points+=reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points=np.mean(total_point_history[-100:])\n",
    "\n",
    "    epsilon=get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('chs-v1.h5')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
